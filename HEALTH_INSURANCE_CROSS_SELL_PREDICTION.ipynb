{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANUJGAUR/HEALTH-INSURANCE-CROSS-SELL-PREDICTION---PROJECT/blob/main/HEALTH_INSURANCE_CROSS_SELL_PREDICTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name    - HEALTH INSURANCE CROSS SELL PREDICTION**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 - Anuj Kumar Gaur**\n",
        "##### **Team Member 2 - Arunkumar Gond**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. There are multiple factors that play a major role in capturing customers for any insurance policy. Here we have information about demographics such as age, gender, region code, and vehicle damage, vehicle age, annual premium, policy sourcing channel. Based on the previous trend, this data analysis and prediction with machine learning models can help us understand what are the reasons for news popularity on social media and obtain the best classification model.\n",
        "\n",
        "We have a dataset which contains information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc. related to a person who is interested in vehicle insurance. We have 381109 data points available.\n",
        "\n",
        "Predicting whether a customer would be interested in buying Vehicle Insurance so that the company can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ANUJGAUR/HEALTH-INSURANCE-CROSS-SELL-PREDICTION---PROJECT"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.**\n",
        "\n",
        "**An insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to an insurance company for this guarantee.**\n",
        "\n",
        "**For example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.**\n",
        "\n",
        "**Just like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called ‘sum assured’) to the customer.**\n",
        "\n",
        "**Building a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.**\n",
        "\n",
        "**Now, in order to predict, whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import missingno as msno\n",
        "import warnings;warnings.simplefilter('ignore')\n",
        "import time\n",
        "\n",
        "# Used in Hypothesis Testing\n",
        "from scipy.stats import *\n",
        "import math\n",
        "\n",
        "# Used in data preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Used in Feature Engineering\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# Used in Data Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Used in Splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Used in Oversampling\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Used in ML Model implementation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Evaluation Metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn import metrics\n",
        "\n",
        "# Hyper Parameter Tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingRandomSearchCV"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Nh4PEDl3J3nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Capstone Project - HEALTH INSURANCE CROSS SELL PREDICTION/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "msno.bar(df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset given is a dataset from Health Insurance industry, and we have to analysis the response of customers and the insights behind it.\n",
        "\n",
        "Sell prediction is analytical studies on the possibility of a customer abandoning service. The goal is to understand and take steps to change it before the costumer gives up the service.\n",
        "\n",
        "The above dataset has 381109 rows and 12 columns. There are no mising values and duplicate values in the dataset. "
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  **id** : Unique ID for the customer\n",
        "*  **Gender** : Gender of the customer\n",
        "\n",
        "*  **Age** : Age of the customer\n",
        "*   **Driving_License 0** : Customer does not have DL, **1** : Customer already has DL\n",
        "\n",
        "*   **Region_Code** : Unique code for the region of the customer\n",
        "*  **Previously_Insured : 1** : Customer already has Vehicle Insurance, **0** : Customer doesn't have Vehicle Insurance\n",
        "\n",
        "*  **Vehicle_Age** : Age of the Vehicle\n",
        "*  **Vehicle_Damage :1** : Customer got his/her vehicle damaged in the past. **0** : Customer didn't get his/her vehicle damaged in the past.\n",
        "\n",
        "*  **Annual_Premium**: The amount customer needs to pay as premium in the year\n",
        "*   **PolicySalesChannel** : Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n",
        "\n",
        "*   **Vintage** : Number of Days, Customer has been associated with the company\n",
        "*  **Response : 1** : Customer is interested, **0** : Customer is not interested\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Assingning numericals column of df to variable numerical_cols\n",
        "numerical_cols = list(df.describe())\n",
        "numerical_df = df[numerical_cols]\n",
        "numerical_df.head()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assingning catagoricals column of df to variable catagorical_cols\n",
        "categorical_cols=list(set(df.columns)-set(numerical_cols))\n",
        "categorical_df=df[categorical_cols]\n",
        "categorical_df.head()"
      ],
      "metadata": {
        "id": "XPsr0Y9PtG8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's check in each column of categorical_df how namy unique values are present\n",
        "for column_name in categorical_cols:\n",
        "  print('-'*35)\n",
        "  print(df[column_name].value_counts(),'\\n')\n",
        "  print('-'*35)"
      ],
      "metadata": {
        "id": "uOoSHds8u9A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='O')"
      ],
      "metadata": {
        "id": "htBWTlv62ZG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset nothing to do much in wrangling i just divide the df in two dfs\n",
        "numerical_df and categorical_df . numerical_df for all the numericals column and categorical_df is for all the categorical columns and i check the categorical_df then i found there is 3 columns gender, vehicle_age and vehicle_damage and their are 2 unique in gender, 3 unique in vehicle_age and 2 unique in vehicle_damage. "
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Countplot on dependent variable i.e., Response (Univariate) "
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Dependent Column Value Counts\n",
        "print(df.Response.value_counts())\n",
        "print(\"\\n\")\n",
        "# Dependent Variable Column Visualization\n",
        "sns.countplot(df.Response)\n",
        "plt.title('Not-Interested vs Interested Policyholders', fontsize=20)\n",
        "plt.show()\n",
        "\n",
        "df.Response.value_counts()/df.shape[0]"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot show the frequency, counts of values for the different levels of a categorical or nominal variable.\n",
        "\n",
        "To show the count of interested and not-interested of policyholders , I have used Countplot."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dependant variable has binary values of 0 and 1. We can infer from the plot above that many clients have no interest in purchasing vehicle insurance. 12.2 percent of the data are 1's and 87.7 percent of the data are 0s. This data must be handled using the imbalance technique since the output feature is unbalanced."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, the gained insights help a positive business impact.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 - Piechart on independent variable i.e., Gender (Univariate)"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Independent Column Value Counts\n",
        "print(df.Gender.value_counts())\n",
        "print(\"\\n\")\n",
        "# Independent Variable Column Visualization\n",
        "df['Gender'].value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               explode=[0,0]\n",
        "                              )\n",
        "plt.title('Distribution of Gender',fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the Gender."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above pie chart i found there is  206089 male i.e., (54.1 %) and 175020 female i.e., (45.9 %). So , we can say that gender variable is almost equally distributed but male are liitle bit more in comparison to female. \n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, the gained insights didn't create a positive business impact.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - Countplot on independent variable i.e., Driving_License (Univariate)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Independent Column Value Counts\n",
        "print(df.Driving_License.value_counts())\n",
        "print('\\n')\n",
        "\n",
        "# Independent Variable Column Visualization\n",
        "sns.countplot(df.Driving_License)\n",
        "plt.title('Driving License Holders vs non-holders',fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot show the frequency, count of values for the different levels of a categorical or nominal variable.\n",
        "\n",
        "To show the Driving License holders vs non holders. I have used countplot."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above countplot i found their are  380297 people those who have the driving license and 812 people are not having driving license. So , we can say that driving license variable is almost equally distributed."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, the gained insights didn't create a positive business impact.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - Countplot on independent variable i.e., Vehicle_Age (Univariate) "
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Independent Column Value Counts\n",
        "print(df.Vehicle_Age.value_counts())\n",
        "print('\\n')\n",
        "\n",
        "# Independent Variable Column Visualization\n",
        "sns.countplot(df.Vehicle_Age)\n",
        "plt.title('Vehicle_Age Distribution',fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot show the frequency, count of values for the different levels of a categorical or nominal variable.\n",
        "\n",
        "To show the Vehicle age distribution. I have used countplot."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above countplot, we can say that most of the people having those vehicle whose age in range 1-2 years in comparison others two."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, the gained insights didn't create a positive business impact.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 - Piechart on independent variable i.e., Vehicle_Damage (Univariate)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Independent Column Value Counts\n",
        "print(df.Vehicle_Damage.value_counts())\n",
        "print('\\n')\n",
        "\n",
        "# Independent Variable Column Visualization\n",
        "df['Vehicle_Damage'].value_counts().plot(kind='pie',\n",
        "                                         figsize=(15,6),\n",
        "                                         autopct='%1.1f%%',\n",
        "                                         startangle=90,\n",
        "                                         shadow=True,\n",
        "                                         explode=(0,0))\n",
        "plt.title('Vehicle_Damage Distribution',fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the Vehicle Damage."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above pie chart i found there are 192413 i.e., (50.5 %) people whose vrhicle is damage and 188696 i.e., (49.5 %) are those people whose vehicle is not damage. So , we can say that Vehicle_Damage variable is almost equally distributed."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, the gained insights didn't create a positive business impact.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - Countplot on independent variable i.e., Previously_Insured (Univariate)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Independent Column Value Counts\n",
        "print(df.Previously_Insured.value_counts())\n",
        "print('\\n')\n",
        "\n",
        "# Independent Variable Column Visualization\n",
        "sns.countplot(df.Previously_Insured)\n",
        "plt.title('Previously_Insured Distribution',fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot show the frequency, count of values for the different levels of a categorical or nominal variable.\n",
        "\n",
        "To show the Vehicle Previously Insured. I have used countplot."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above countplot, i found that there ar 206481 those people who not insured previously and 174628 are those people who insured previously, which is almost equally distributed but Previously Insured people are little bit more in comparison to not insured people.  "
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, the gained insights didn't create a positive business impact.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Countplot on response based on gender (Bivariate)"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "#Analyzing responses based on gender\n",
        "plt.figure(figsize = (12,5))\n",
        "sns.countplot(df['Gender'], hue= df['Response'])\n",
        "plt.title('Response in Male and Female category', fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot show the frequency, count of values for the different levels of a categorical or nominal variable.\n",
        "\n",
        "To show the response on gender. I have used countplot."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above countplot, We can see that males are more likely to purchase vehicle insurance."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights help to create a positive impact on business because the males are more likely to purchase vehicle insurance.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - Countplot on Age Vs Response (Bivariate)"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize = (20,10))\n",
        "sns.countplot(x='Age', hue='Response', data=df)\n",
        "plt.title('Response in terms of Age', fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot show the frequency, count of values for the different levels of a categorical or nominal variable.\n",
        "\n",
        "To show the Age Vs Response. I have used countplot."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above countplot,\n",
        "*   We can see that Ages below 30 are not more interested in purchasing vehicle \n",
        "insurance may be because lack of experience and maturity levels.\n",
        "*   People who are above 30-60 are more likely to be interested.\n",
        "\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights help to create a positive impact on business because on the basis of above analysis we can target those peoples whose age in between 30-60.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Countplot on Driving License Vs Response (Bivariate)"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='Driving_License',hue='Response',data=df)\n",
        "plt.title('Driving License Vs Response',fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Countplot show the frequency, count of values for the different levels of a categorical or nominal variable.\n",
        "\n",
        "To show the Driving License Vs Response. I have used countplot."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above countplot, we can say that, Customers who are interested in Vehicle insurance are almost having the driving license."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights help to create a positive impact on business because on the basis of above analysis we can target those peoples who are having driving license.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - Countplot on Previously Insured Vs Response (Bivariate)."
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "#In Previously_Insured 1 means Customer already has Vehicle Insurance and 0 means Customer doesn't have Vehicle Insurance yet\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.countplot(x=\"Previously_Insured\",hue =\"Response\",data=df)\n",
        "plt.title('Response in terms of Previously Insured ', fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot show the frequency, count of values for the different levels of a categorical or nominal variable.\n",
        "\n",
        "To show the Previously Insured Vs Response. I have used countplot."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above count-plot, we found that their are almost everyone is purchase insurence those are previously insured."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights help to create a positive impact on business because on the basis of above analysis we can target those peoples who are having Previously insured.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Countplot on Vehicle age Vs Response (Bivariate)"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='Vehicle_Age',hue='Response',data=df)\n",
        "plt.title('Vehicle age Vs Response',fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot show the frequency, count of values for the different levels of a categorical or nominal variable.\n",
        "\n",
        "To show the Previously Insured Vs Response. I have used countplot."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above graph, we can say that customers whose vehicle age is between 1-2 years are most likely to be interested when compared to the other two."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights help to create a positive impact on business because on the basis of above analysis, we have a lower number of policyholders with vehicles older than two years, so we must focus more on the other two categories.\n",
        ".\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Countplot on Region code Vs Response (Bivariate)"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "plt.figure(figsize=(22,6))\n",
        "sns.countplot(x='Region_Code',hue='Response',data=df)\n",
        "plt.title('Region code Vs Response',fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot show the frequency, count of values for the different levels of a categorical or nominal variable.\n",
        "\n",
        "To show the Previously Insured Vs Response. I have used countplot."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above graph, i found that the Region Code - 0.28 has more customers in comparison to others region codes."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights help to create a positive impact on business because on the basis of above analysis, we can target the people of region code 0.28.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Countplot on Vehicle_Damage Vs Response (Bivariate)"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='Vehicle_Damage',hue='Response',data=df)\n",
        "plt.title('Vehicle_Damage Vs Response')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot show the frequency, count of values for the different levels of a categorical or nominal variable.\n",
        "\n",
        "To show the Vehicle Damage Vs Response. I have used countplot."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can infer from the above plot that those people whose vehicle is damage are taking insurence more in comparison those who's vehicle are not damaged."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights help to create a positive impact on business because on the basis of above analysis, we can target those people whose vehicles are damaged.\n",
        "\n",
        "No, there are no such insights that lead to negative growth."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "#Checking correlation of all the columns using heatmap\n",
        "plt.figure(figsize=(18,10))\n",
        "correlation = df.corr()\n",
        "sns.heatmap(abs(correlation),annot=True,linewidth=3,cmap='coolwarm')\n",
        "plt.title('Pearson correlation of Features',fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses. The range of correlation is [-1,1].\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, i used correlation heatmap."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations based on correlation plot:-\n",
        "\n",
        "Target variable ( Response ) is not much affected by Vintage variable. we can drop least correlated variable.\n",
        "\n",
        "From the above correlation heatmap, we can see that the policy_sales_channel and age is negatively correlated.\n",
        "\n",
        "Rest all correlation can be depicted from the above chart."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# Commenting this code because it take too much time to run\n",
        "'''\n",
        "sns.pairplot(df,hue='Response')\n",
        "plt.title('Relation between each variables',fontsize=20)\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our data-set.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation -\n",
        "\n",
        "The graph above shows how each feature is distributed in respect to other features. Since many features have binary values, we cannot see a good relationship with other features. Due to more unique values,some of the features are uniformly distributed. The premium feature has a skewed relationship with the other features since it is skewed to the right."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While performing the EDA i was thinking about these following statements, these are just my thought you may come with more ;\n",
        "\n",
        "\n",
        "1.   If customer buying Health insurance then on an average customer taking Annual premium of rs. 30565\n",
        "2.   If customer buying Health insurance then atleast 154 days, customer has been assosiated with the company. \n",
        "\n",
        "3.   At most 60 years of customers are more likely to buy the insurance.\n",
        "\n"
      ],
      "metadata": {
        "id": "eMYPQagQiDCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Parameter Class \n",
        "class findz:\n",
        "  def proportion(self,sample,hyp,size):\n",
        "    return (sample - hyp)/math.sqrt(hyp*(1-hyp)/size)\n",
        "  def mean(self,hyp,sample,size,std):\n",
        "    return (sample - hyp)*math.sqrt(size)/std\n",
        "  def varience(self,hyp,sample,size):\n",
        "    return (size-1)*sample/hyp\n",
        "\n",
        "variance = lambda x : sum([(i - np.mean(x))**2 for i in x])/(len(x)-1)\n",
        "zcdf = lambda x: norm(0,1).cdf(x)\n",
        "# Creating a function for getting P value\n",
        "def p_value(z,tailed,t,hypothesis_number,df,col):\n",
        "  if t!=\"true\":\n",
        "    z=zcdf(z)\n",
        "    if tailed=='l':\n",
        "      return z\n",
        "    elif tailed == 'r':\n",
        "      return 1-z\n",
        "    elif tailed == 'd':\n",
        "      if z>0.5:\n",
        "        return 2*(1-z)\n",
        "      else:\n",
        "        return 2*z\n",
        "    else:\n",
        "      return np.nan\n",
        "  else:\n",
        "    z,p_value=stats.ttest_1samp(df[col],hypothesis_number)\n",
        "    return p_value\n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "# Conclusion about the P - Value\n",
        "def conclusion(p):\n",
        "  significance_level = 0.05\n",
        "  if p>significance_level:\n",
        "    return f\"Failed to reject the Null Hypothesis for p = {p}.\"\n",
        "  else:\n",
        "    return f\"Null Hypothesis rejected Successfully for p = {p}\"\n",
        "\n",
        "# Initializing the class\n",
        "findz = findz()"
      ],
      "metadata": {
        "id": "jKKE7-vXbxnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Taking sample from dataset\n",
        "sample = df.sample(frac=0.25)"
      ],
      "metadata": {
        "id": "pPX7n469ZKGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If customer buying Health insurance then on an average customer taking Annual premium of rs. 30565."
      ],
      "metadata": {
        "id": "Wqh59FT9dWD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: N = 30565\n",
        "\n",
        "Alternate Hypothesis : N != 30565\n",
        "\n",
        "Test Type: Two Tailed Test"
      ],
      "metadata": {
        "id": "8TqCKkLjczt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "wym67iawEFqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the required parameter values for hypothesis testing\n",
        "hypothesis_number = 30565\n",
        "sample_mean_1 = sample[\"Annual_Premium\"].mean()\n",
        "size = len(sample)\n",
        "std_1=(variance(sample[\"Annual_Premium\"]))**0.5"
      ],
      "metadata": {
        "id": "QB_QYi09ECAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Getting Z value\n",
        "z = findz.mean(hypothesis_number,sample_mean_1,size,std_1)\n",
        "# Getting P - Value\n",
        "p = p_value(z=z,tailed='d',t=\"true\",hypothesis_number=hypothesis_number,df=sample,col=\"Annual_Premium\")\n",
        "# Getting Conclusion\n",
        "print(conclusion(p))"
      ],
      "metadata": {
        "id": "gph_T5gnE8l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "h-UGmttrFCOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used T-Test as the statistical testing to obtain P-Value and found the result that Failed to reject Null hypothesis and Customers buying health insurance have the average of Annual Premium is 30565."
      ],
      "metadata": {
        "id": "LJmbuPPQFIME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "TTn54l6ZFNMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing code of hist plot for required columns to know the data distibution\n",
        "\n",
        "fig=plt.figure(figsize=(9,6))\n",
        "ax=fig.gca()\n",
        "feature= (sample[\"Annual_Premium\"])\n",
        "sns.distplot(sample[\"Annual_Premium\"])\n",
        "ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "ax.set_title('c')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wJr5YNbqGuR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_median_difference=sample[\"Annual_Premium\"].median()- sample[\"Annual_Premium\"].mean()\n",
        "print(\"Mean Median Difference is :-\",mean_median_difference)"
      ],
      "metadata": {
        "id": "XWiiQUORHDoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above we can see the differemnce between mean and median is larger and the distribution is postively skewed. For a skewed data Z-Test can't be performed.\n",
        "\n",
        "Non-parametric tests are most useful for small studies. Using non-parametric tests in large studies may provide answers to the wrong question, thus confusing readers. For studies with a large sample size, t-tests and their corresponding confidence intervals can and should be used even for heavily skewed data.\n",
        "\n",
        "So, for a skewed data we can use T-test for better result. Thus, I used t - test."
      ],
      "metadata": {
        "id": "REucDYd5FOjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If customer buying Health insurance then atleast 154 days, customer has been assosiated with the company."
      ],
      "metadata": {
        "id": "LEc88t-bJKe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: N >= 154\n",
        "\n",
        "Alternate Hypothesis : N < 154\n",
        "\n",
        "Test Type: Left Tailed Test"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the required parameter values for hypothesis testing\n",
        "hypothesis_number = 154\n",
        "sample_mean_2 = sample[\"Vintage\"].mean()\n",
        "size = len(sample)\n",
        "std_2=(variance(sample[\"Vintage\"]))**0.5"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Getting Z value\n",
        "z = findz.mean(hypothesis_number,sample_mean_2,size,std_2)\n",
        "# Getting P - Value\n",
        "p = p_value(z=z,tailed='l',t=\"false\",hypothesis_number=hypothesis_number,df=sample,col=\"Vintage\")\n",
        "# Getting Conclusion\n",
        "print(conclusion(p))"
      ],
      "metadata": {
        "id": "doQ_p9pxKp66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Z-Test as the statistical testing to obtain P-Value and found the result that failed to reject Null hypothesis and Customers are assosiated with the company atleast 154 days after buying health insurance."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing code of hist plot for required columns to know the data distibution\n",
        "\n",
        "fig=plt.figure(figsize=(9,6))\n",
        "ax=fig.gca()\n",
        "feature= (sample[\"Vintage\"])\n",
        "sns.distplot(sample[\"Vintage\"])\n",
        "ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "ax.set_title('col')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SkgFQ8NQLOMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_median_difference=sample[\"Vintage\"].median()- sample[\"Vintage\"].mean()\n",
        "print(\"Mean Median Difference is :-\",mean_median_difference)"
      ],
      "metadata": {
        "id": "hXoGgBI4L1gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in the figure the mean is approximately same as the median. Thus, it is a Normal Distribution. That's why I have used Z-Test directly."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At most 60 years of customers are more likely to buy the insurance."
      ],
      "metadata": {
        "id": "ZdfACbukMjxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: N <= 60\n",
        "\n",
        "Alternate Hypothesis : N > 60\n",
        "\n",
        "Test Type: Right Tailed Test"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the required parameter values for hypothesis testing\n",
        "hypothesis_number = 60\n",
        "sample_mean_3 = sample[\"Age\"].mean()\n",
        "size = len(sample)\n",
        "std_3=(variance(sample[\"Age\"]))**0.5"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Getting Z value\n",
        "z = findz.mean(hypothesis_number,sample_mean_3,size,std_3)\n",
        "# Getting P - Value\n",
        "p = p_value(z=z,tailed='r',t=\"false\",hypothesis_number=hypothesis_number,df=sample,col=\"Age\")\n",
        "# Getting Conclusion\n",
        "print(conclusion(p))"
      ],
      "metadata": {
        "id": "zsLDtoVeOeDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Z-Test as the statistical testing to obtain P-Value and found the result that Null hypothesis can't be rejected and Customers at most age 60 are buying health insurance."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing code of hist plot for required columns to know the data distibution\n",
        "\n",
        "fig=plt.figure(figsize=(9,6))\n",
        "ax=fig.gca()\n",
        "feature= (sample[\"Age\"])\n",
        "sns.distplot(sample[\"Age\"])\n",
        "ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "ax.set_title('col')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s6MIDx8CipTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_median_difference=sample[\"Age\"].median()- sample[\"Age\"].mean()\n",
        "print(\"Mean Median Difference is :-\",mean_median_difference)"
      ],
      "metadata": {
        "id": "S4n0ZKXliZeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in the figure the mean is approximately same as the median. Thus, it is a Normal Distribution. That's why I have used Z-Test directly."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of the dataset for further feature engineering\n",
        "df_1 = df.copy()"
      ],
      "metadata": {
        "id": "PtKMdfOlvnMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Missing Values/Null Values Count\n",
        "print(df_1.isnull().sum())"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing values to handle in the given dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Checking Outliers with boxplot\n",
        "for column_name in numerical_cols:\n",
        "  plt.figure(figsize=(12,6))\n",
        "  sns.boxplot(df_1[column_name])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for checking outliers\n",
        "def check_outlier(start,end,step,column_name):\n",
        "  for i in range(start,end,step):\n",
        "    n = column_name.values\n",
        "    n = np.sort(n)\n",
        "    print('{} percentile value is {}'.format(i,int(column_name.quantile(i/100))))\n",
        "  print('{} percentile value is {}'.format(end,int(column_name.quantile(end/100))))"
      ],
      "metadata": {
        "id": "hLMeA-WLnPCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking outliers of Annual_Premium\n",
        "check_outlier(0,100,10,df_1['Annual_Premium'])"
      ],
      "metadata": {
        "id": "3d2-3O7-rKUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking outliers of Annual_Premium\n",
        "check_outlier(10,20,1,df_1['Annual_Premium'])"
      ],
      "metadata": {
        "id": "1OjglcNKuxAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking outliers of Annual_Premium\n",
        "check_outlier(90,100,1,df_1['Annual_Premium'])"
      ],
      "metadata": {
        "id": "RhnMRimS0QCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing outliers of column Annual_Premium\n",
        "df_1 = df_1.drop(df_1[df['Annual_Premium'] < 19050].index)\n",
        "df_1 = df_1.drop(df_1[df['Annual_Premium'] > 60680].index)"
      ],
      "metadata": {
        "id": "Y6P5qP5avSPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Again checking outliers are removed or not\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(df_1['Annual_Premium'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lFChVaCPzSWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used 2 outlier treatment techniques to check outliers boxplot and quantile because in these techniques i can easily find the outliers after which value it started."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Label Encoder on Vehicle damage\n",
        "le = LabelEncoder()\n",
        "df_1['Vehicle_Damage'] = le.fit_transform(df_1['Vehicle_Damage'])"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing categorical value to numerical values\n",
        "df_1 = pd.get_dummies(df_1, columns = ['Gender','Vehicle_Age'])"
      ],
      "metadata": {
        "id": "qJ8Qg7ioKX7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1.head()"
      ],
      "metadata": {
        "id": "kYE7lN6WCV7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Label Encoding technique for Vehicle_Damage column and One Hot Encoding for Gender, Vehicle_Age Column. I have used label encoding, because there are only 2 unique values (Yes,No) in column  which needed to be encoded.\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "#Contain all independent variables\n",
        "x = df_1.drop(['Response'], axis=1)\n",
        "\n",
        "#Contain Dependent variable\n",
        "y = df_1['Response'] "
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation Variance Threshold\n",
        "variance_threshold = VarianceThreshold(threshold=0)\n",
        "variance_threshold.fit(df_1)\n",
        "variance_threshold.get_support()"
      ],
      "metadata": {
        "id": "wKfL3GhnxYs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation ExtraTreesClassifier\n",
        "extra_tree_forest = ExtraTreesClassifier(n_estimators = 5,criterion ='entropy', max_features = 2)\n",
        "\n",
        "# Training the model\n",
        "extra_tree_forest.fit(x, y)\n",
        "\n",
        "# Computing the importance of each feature\n",
        "feature_importance = extra_tree_forest.feature_importances_"
      ],
      "metadata": {
        "id": "o8gmEFQ6mrAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the individual importances\n",
        "feature_importance_normalized = np.std( [ tree.feature_importances_ for tree in extra_tree_forest.estimators_ ] , axis = 0)"
      ],
      "metadata": {
        "id": "FnRMMfDHm0Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting a Bar Graph to compare the models\n",
        "plt.figure(figsize = (24,10))\n",
        "plt.bar(x.columns, feature_importance_normalized)\n",
        "plt.xlabel('Feature Labels' , fontsize = 15)\n",
        "plt.ylabel('Feature Importances' , fontsize = 15)\n",
        "plt.title('Comparison of different Feature Importances' , fontsize = 25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hw8GJ2wfleYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_importances_Series = pd.Series( feature_importance_normalized , index=x.columns)\n",
        "print(\"Feature Name\\t\\t Importance\")\n",
        "print(\"-\"*37 , end='\\n')\n",
        "feat_importances_Series.sort_values()"
      ],
      "metadata": {
        "id": "sQ_3XbRhlx20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping less important features\n",
        "df_1.drop(columns=['Driving_License','Gender_Female','Gender_Male','id'],inplace=True)"
      ],
      "metadata": {
        "id": "nRFCaiUlmm51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "#Checking correlation of all the columns using heatmap\n",
        "plt.figure(figsize = (18,10))\n",
        "correlation = df_1.corr()\n",
        "sns.heatmap(correlation, annot= True,linewidths=3,cmap='coolwarm')\n",
        "plt.title(\"Pearson correlation of Features\", y=1.05, size=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZzkGtLlsnL0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used three method for feature selection Variance threshold, Extra Tree classification and correlation map. I used Variance threshold to check the which columns has constant values, Tree classification for checking whuch columns are less important for the dependent variable and correlation map for the removing of high correlation variables."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Age',\n",
        " 'Region_Code',\n",
        " 'Previously_Insured',\n",
        " 'Vehicle_Age',\n",
        " 'Vehicle_Damage',\n",
        " 'Annual_Premium',\n",
        " 'Policy_Sales_Channel','Vehicle_Age_1-2 Year','Vehicle_Age_< 1 Year','Vehicle_Age_> 2 Year'\n",
        "\n",
        " I found these features are imprtant from the above obervation."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data Splitting"
      ],
      "metadata": {
        "id": "0twPExQoqpBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Contain all independent variables\n",
        "X = df_1.drop(['Response'], axis=1)\n",
        "\n",
        "#Contain Dependent variable\n",
        "Y = df_1['Response'] "
      ],
      "metadata": {
        "id": "9HQM-zOOq-ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
      ],
      "metadata": {
        "id": "LE0gFiMlqzu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train Data Shape')\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print('\\n')\n",
        "print('Test Data Shape')\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "kL_61qI-rLiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "nO1uKectreFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used 80% data for training and 20% of data for test because it gives the best results."
      ],
      "metadata": {
        "id": "DYdBJ5CormBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "xQJEUENMrzMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Dependant Column Value Counts\n",
        "print(df_1.Response.value_counts())\n",
        "print(\" \")\n",
        "# Dependant Variable Column Visualization\n",
        "df_1['Response'].value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               colors=['skyblue','red'],\n",
        "                               explode=[0,0]\n",
        "                              )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bu7oi4LosXiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "ACgebNXvsAS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most significant challenges when dealing with unbalanced datasets is the metrics used to evaluate their model. Using simpler metrics, such as accuracy score, can be misleading. In a dataset with highly unbalanced classes, the classifier will always \"predict\" the most common class without performing any feature analysis, and while it will have a high accuracy rate, it will often be incorrect."
      ],
      "metadata": {
        "id": "B-2qxJRksB6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# using Smote Oversampling\n",
        "sm = SMOTE(random_state=5)\n",
        "X_train, y_train = sm.fit_resample(x_train, y_train)"
      ],
      "metadata": {
        "id": "zUPKE08RsA69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Shape after resampling\n",
        "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)"
      ],
      "metadata": {
        "id": "4aeI5_XNtkNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "K_dzTWG1uM6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has now been balanced using the oversampling technique, and it is ready for training the model.we have used smote and oversampling technique. oversampling performing better."
      ],
      "metadata": {
        "id": "aSl6a7CguTf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Standardization (StandardScaler) method for Data Scaling.\n",
        "\n",
        "Standardization Here all the features will be transformed in such a way that it will have the properties of a standard normal distribution with mean (μ) = 0 and standard deviation(σ)=l.  "
      ],
      "metadata": {
        "id": "rHV5Oj4Zp5Uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem can be identified as Binary Classification (wheather customer opts for vehicle insurance or not)\n",
        "\n",
        "Dataset has more than 300k records\n",
        "\n",
        "cannot go with SVM Classifier as it takes more time to train as dataset increase\n",
        "\n",
        "Idea is to start selection of models as:\n",
        "\n",
        " 1.Logistic Regression\n",
        "\n",
        " 2.Decision Tree\n",
        "\n",
        " 3.Gaussian Naive Bayes\n",
        "\n",
        "4.AdaBoost Classifier\n",
        "\n",
        " 5.XGBClassifier\n",
        "\n",
        "6.Bagging Classifier\n",
        "\n",
        "7.Lightgbm Classifier"
      ],
      "metadata": {
        "id": "7g01UVFKzIe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions used in ML implementation"
      ],
      "metadata": {
        "id": "j-kkc6OCsOsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for ploting confusion matrix \n",
        "def plot_confusion_matrix(X_test, y_test, y_pred_test):\n",
        "    \n",
        "    label = ['1', '0']\n",
        "    cm = confusion_matrix(y_test, y_pred_test)\n",
        "    group_names = ['True positive','False Positive','False Negative','True Negative']\n",
        "    group_counts = ['{0:0.0f}'.format(value) for value in\n",
        "                    cm.flatten()]\n",
        "    group_percentages = ['{0:.2%}'.format(value) for value in\n",
        "                        cm.flatten()/np.sum(cm)]\n",
        "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
        "            zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=labels, fmt='',cmap='Blues')\n",
        "    ax.set_title('Confusion Matrix', fontdict={'fontsize': 16, 'fontweight':'bold'})\n",
        "    ax.xaxis.set_ticklabels(label)\n",
        "    ax.yaxis.set_ticklabels(label)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sszD3vRMx4i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for ploting roc curve\n",
        "def plot_roc_curve(model, X_test, y_test, y_pred):\n",
        "\n",
        "    pred_proba = model.predict_proba(X_test)\n",
        "    # roc curve for models\n",
        "    fpr, tpr, thresh = roc_curve(y_test, pred_proba[:,1], pos_label=1)     \n",
        "    # roc curve for tpr = fpr \n",
        "    random_probs = [0 for i in range(len(y_test))]\n",
        "    p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n",
        "    # plot roc curves\n",
        "    plt.plot(fpr, tpr,linestyle='--',color='red', label = type(model).__name__)\n",
        "    plt.plot(p_fpr, p_tpr, linestyle='-', color='blue')\n",
        "    # title\n",
        "    plt.title('ROC curve', fontdict={'fontsize': 16, 'fontweight':'bold'})\n",
        "    # x label\n",
        "    plt.xlabel('False Positive Rate', fontdict={'fontsize': 12})\n",
        "    # y label\n",
        "    plt.ylabel('True Positive rate', fontdict={'fontsize': 12})\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pdNfA94j8a81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function Evaluation matrices before tuning\n",
        "models = []\n",
        "Train_Accuracy=[]\n",
        "Train_Precision=[]\n",
        "Train_Recall=[]\n",
        "Train_F1_Score=[]\n",
        "Train_ROC_AUC_Score=[]\n",
        "Train_Log_Loss=[]\n",
        "def Performance(model,y_tset, y_pred):\n",
        "  models.append(type(model).__name__)\n",
        "  acs = accuracy_score(y_tset, y_pred)\n",
        "  Train_Accuracy.append(accuracy_score(y_test,y_pred))\n",
        "  ps = precision_score(y_tset, y_pred)\n",
        "  Train_Precision.append(precision_score(y_test,y_pred))\n",
        "  rs = recall_score(y_tset, y_pred)\n",
        "  Train_Recall.append(recall_score(y_test,y_pred))\n",
        "  f1s = f1_score(y_tset, y_pred)\n",
        "  Train_F1_Score.append(f1_score(y_test,y_pred))\n",
        "  ras = roc_auc_score(y_tset, y_pred)\n",
        "  Train_ROC_AUC_Score.append(roc_auc_score(y_test, y_pred))\n",
        "  ll = log_loss(y_test, y_pred)\n",
        "  Train_Log_Loss.append(log_loss(y_test, y_pred))\n",
        "  print('*'*115)\n",
        "  print('Accuracy Score','    Train_Precision Score','    Train_Recall Score','      F1 Score','           ROC AUC Score','     Log Loss')\n",
        "  print(acs, ps, rs,f1s, ras,ll)"
      ],
      "metadata": {
        "id": "JvYvpE3CYhGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for Evaluation Metrics after tuning\n",
        "Accuracy=[]\n",
        "Precision=[]\n",
        "Recall=[]\n",
        "F1_Score=[]\n",
        "ROC_AUC_Score=[]\n",
        "Log_Loss=[]\n",
        "def evaluation_metrics(model,y_test,y_pred):\n",
        "    metrics_dict = {}\n",
        "    metrics_dict['Accuracy_Score'] = [accuracy_score(y_test,y_pred)]  #Accuracy Score\n",
        "    Accuracy.append(accuracy_score(y_test,y_pred))\n",
        "    metrics_dict['Precision'] = [precision_score(y_test,y_pred)] #Precision\n",
        "    Precision.append(precision_score(y_test,y_pred))\n",
        "    metrics_dict['Recall'] = [recall_score(y_test,y_pred)] #Recall\n",
        "    Recall.append(recall_score(y_test,y_pred))\n",
        "    metrics_dict['F1_Score'] = [f1_score(y_test,y_pred)] #F1 Score\n",
        "    F1_Score.append(f1_score(y_test,y_pred))\n",
        "    metrics_dict['ROC_AUC_Score'] = [roc_auc_score(y_test, y_pred)] #ROC AUC Score\n",
        "    ROC_AUC_Score.append(roc_auc_score(y_test, y_pred))\n",
        "    metrics_dict['Log_Loss'] = [log_loss(y_test, y_pred)] #Log Loss\n",
        "    Log_Loss.append(log_loss(y_test, y_pred))\n",
        "    metrics_df = pd.DataFrame(metrics_dict)\n",
        "    print('*'*75)\n",
        "    print(metrics_df)"
      ],
      "metadata": {
        "id": "GVKLXsnF8hRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for hyperparameter tunning\n",
        "def hyperparameter_tuning(x_train, y_train, model, parameters, tuning_model):\n",
        "\n",
        "    if tuning_model == 'Halving_Randomized_Search_CV':\n",
        "        tuned_model = HalvingRandomSearchCV(model, param_distributions = parameters, scoring = \"accuracy\", n_jobs=-1, factor=3, cv = 5 )\n",
        "    \n",
        "    elif tuning_model == 'Randomized_Search_CV':\n",
        "        tuned_model = RandomizedSearchCV(model, param_distributions = parameters, scoring = 'accuracy', cv = 3, n_iter = 50, n_jobs=-1)\n",
        "\n",
        "    else:\n",
        "        tuned_model = GridSearchCV(model, param_grid = parameters, scoring = 'accuracy', n_jobs=-1, cv = 3)\n",
        "\n",
        "    print('########'*8+'\\n     <<<< '+f'Tuning Model: {tuning_model}'+' >>>>\\n'+'********'*8)\n",
        "    print('-----'*10+f'\\n{type(model).__name__}\\n'+'-----'*10)\n",
        "    \n",
        "    start_time = time.time() \n",
        "    \n",
        "    tuned_model.fit(x_train, y_train)\n",
        "    \n",
        "    stop_time = time.time()\n",
        "\n",
        "    print('*****'*10+f'\\nBest Score for {type(model).__name__} : {tuned_model.best_score_}','\\n---')\n",
        "    print(f'Best Parameters for {type(model).__name__} : {tuned_model.best_params_}\\n'+'-----'*10)\n",
        "\n",
        "    print('Elapsed Time:',time.strftime(\"%H:%M:%S\", time.gmtime(stop_time - start_time)))\n",
        "    print('======'*5)\n",
        "\n",
        "    return tuned_model"
      ],
      "metadata": {
        "id": "RB2E3cvpSJ9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - **Implementing Logistic Regression**"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "model1 = LogisticRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "model1 = model1.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_class_preds = model1.predict(X_train)\n",
        "test_class_preds = model1.predict(X_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics before tuning\n",
        "print('-----'*23+f'\\nEvaluation of {type(model1).__name__} before tuning:\\n'+'-----'*23)\n",
        "Performance(model1,y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "1cjUFH23xxJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, test_class_preds)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model1, X_test, y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "parameters_logistic = {'solver' : ['newton-cg', 'lbfgs', 'liblinear','sag','saga'],\n",
        "                        'penalty' : ['l2'],\n",
        "                        'C' : [100, 10, 1.0, 0.1, 0.01, 0.001], \n",
        "                       'random_state':[2]}\n",
        "# Fit the Algorithm\n",
        "tuned_model = hyperparameter_tuning(X_train, y_train, model1, parameters_logistic, 'Halving_Randomized_Search_CV')\n",
        "\n",
        "# Predict on the model\n",
        "tuned_pred = tuned_model.predict(X_test)\n",
        "\n",
        "# Evaluation Metrics after tuning\n",
        "print(f'\\nEvaluation of {type(model1).__name__} after tuning:\\n'+'-----'*10)\n",
        "evaluation_metrics(model1,y_test,tuned_pred)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, tuned_pred)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model1, X_test, y_test, tuned_pred)"
      ],
      "metadata": {
        "id": "BDB32fYQndrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used 'Halving_Randomized_Search_CV' hyperparameter optimization technique because it very less time in comparison to others techniques. "
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have seen No improvement after tunning."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - **Implementing DecisionTreeClassifier**"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation (DecisionTreeClassifier)\n",
        "model2 = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "model2 = model2.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_class_preds = model2.predict(X_train)\n",
        "test_class_preds = model2.predict(X_test)"
      ],
      "metadata": {
        "id": "Of1p06sKpmpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics before tuning\n",
        "print('-----'*23+f'\\nEvaluation of {type(model2).__name__} before tuning:\\n'+'-----'*23)\n",
        "Performance(model2,y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, test_class_preds)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model2, X_test, y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "M37q48lkqMGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "parameters_decision_tree = {\"splitter\":[\"best\",\"random\"],\n",
        "            \"max_depth\" : [None,5,7,9],\n",
        "           \"min_samples_leaf\":[1,2,3,4,5],\n",
        "           \"min_weight_fraction_leaf\":[0.0, 0.3,0.4,0.5],\n",
        "           \"max_features\":[\"auto\",\"log2\",\"sqrt\",None],\n",
        "           \"max_leaf_nodes\":[None,30,40,50,60], \n",
        "           'random_state':[23]}\n",
        "                       \n",
        "# Fit the Algorithm\n",
        "tuned_model = hyperparameter_tuning(X_train, y_train, model2, parameters_decision_tree, 'Halving_Randomized_Search_CV')\n",
        "\n",
        "# Predict on the model\n",
        "tuned_pred = tuned_model.predict(X_test)\n",
        "\n",
        "# Evaluation Metrics after tuning\n",
        "print(f'\\nEvaluation of {type(model2).__name__} after tuning:\\n'+'-----'*10)\n",
        "evaluation_metrics(model2,y_test,tuned_pred)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, tuned_pred)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model1, X_test, y_test, tuned_pred)"
      ],
      "metadata": {
        "id": "LwtGMuOzqHJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used 'Halving_Randomized_Search_CV' hyperparameter optimization technique because it very less time in comparison to others techniques."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the hyperparameter tuning i have seen the model is under performing, its performing well before tuning . so, no need of tuning in this model."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - **Implementing GaussianNaiveBayes**\n",
        "\n"
      ],
      "metadata": {
        "id": "ORIEh4wlswT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation (GaussianNaiveBayes)\n",
        "model4 = GaussianNB()\n",
        "\n",
        "# Fit the Algorithm\n",
        "model4 = model4.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_class_preds = model4.predict(X_train)\n",
        "test_class_preds = model4.predict(X_test)"
      ],
      "metadata": {
        "id": "xC_zxjAftNJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "MdoNJt1StgfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics before tuning\n",
        "print('-----'*23+f'\\nEvaluation of {type(model4).__name__} before tuning:\\n'+'-----'*23)\n",
        "Performance(model4,y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "6fVKtlzDtmyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, test_class_preds)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model4, X_test, y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "YWecI2uuttk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "CBQIXBd9tzIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "parameters_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
        "                    \n",
        "# Fit the Algorithm\n",
        "tuned_model = hyperparameter_tuning(X_train, y_train, model4, parameters_NB, 'Halving_Randomized_Search_CV')\n",
        "\n",
        "# Predict on the model\n",
        "tuned_pred = tuned_model.predict(X_test)\n",
        "\n",
        "# Evaluation Metrics after tuning\n",
        "print(f'\\nEvaluation of {type(model4).__name__} after tuning:\\n'+'-----'*10)\n",
        "evaluation_metrics(model4,y_test,tuned_pred)"
      ],
      "metadata": {
        "id": "vhTKLzzkt5-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, tuned_pred)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model1, X_test, y_test, tuned_pred)"
      ],
      "metadata": {
        "id": "sdCYwvZCqfeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "hliyQcotuQVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used 'Halving_Randomized_Search_CV' hyperparameter optimization technique because it very less time in comparison to others techniques."
      ],
      "metadata": {
        "id": "zr6xJrsvudI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "W30AzclouYu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this model after the hyperparameter tuning i have slightly difference that is near to negligeble in this model also if we don't do tuning it will work same as before tuning. "
      ],
      "metadata": {
        "id": "Qv0P5BlSugr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 - **Implementing AdaBoost Classifier**\n",
        "\n"
      ],
      "metadata": {
        "id": "YsdniOO7usPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5 Implementation (AdaBoostClassifier)\n",
        "model5 = AdaBoostClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "model5 = model5.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_class_preds = model5.predict(X_train)\n",
        "test_class_preds = model5.predict(X_test)"
      ],
      "metadata": {
        "id": "o8qWoWM3usPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "BlqXvuuSusPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics before tuning\n",
        "print('-----'*23+f'\\nEvaluation of {type(model5).__name__} before tuning:\\n'+'-----'*23)\n",
        "Performance(model5,y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "olkjf7OTusPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, test_class_preds)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model5, X_test, y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "Tk0FpJ0vusPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "3zQZq8YuusPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "parameters_ada = {'n_estimators':[10, 100, 200,400],\n",
        "              'learning_rate':[0.001, 0.01, 0.1, 0.2, 0.5],\n",
        "              'random_state':[2]}\n",
        "# Fit the Algorithm\n",
        "tuned_model = hyperparameter_tuning(X_train, y_train, model5, parameters_ada, 'Halving_Randomized_Search_CV')\n",
        "\n",
        "# Predict on the model\n",
        "tuned_pred = tuned_model.predict(X_test)\n",
        "\n",
        "# Evaluation Metrics after tuning\n",
        "print(f'\\nEvaluation of {type(model5).__name__} after tuning:\\n'+'-----'*10)\n",
        "evaluation_metrics(model5,y_test,tuned_pred)"
      ],
      "metadata": {
        "id": "vEuKvDKgusPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, tuned_pred)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model1, X_test, y_test, tuned_pred)"
      ],
      "metadata": {
        "id": "RkneWdX6qpiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "F6VtV3eXusPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used 'Halving_Randomized_Search_CV' hyperparameter optimization technique because it very less time in comparison to others techniques."
      ],
      "metadata": {
        "id": "fnISJwl2usPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "uU0e9FwousPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the hyperparameter tuning i have seen the model is performing wrost and log loss is 13, its performing well before tuning . so, no need of tuning in this model."
      ],
      "metadata": {
        "id": "bGkln1uYusPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5 - **Implementing XGBClassifier**\n",
        "\n"
      ],
      "metadata": {
        "id": "iJcPgeLevyWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 6 Implementation (XGBClassifier)\n",
        "model6 = XGBClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "model6 = model6.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_class_preds = model6.predict(X_train)\n",
        "test_class_preds = model6.predict(X_test)"
      ],
      "metadata": {
        "id": "WE7fqkNpvyWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "UcZpdeRYvyWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics before tuning\n",
        "print('-----'*23+f'\\nEvaluation of {type(model6).__name__} before tuning:\\n'+'-----'*23)\n",
        "Performance(model6,y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "xJRdg_hQvyWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, test_class_preds)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model6, X_test, y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "-4Vd06-vvyWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "r_FtcxIJvyWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "parameters_xgb = {'random_state':[5],'max_depth':[5]}\n",
        "                    \n",
        "# Fit the Algorithm\n",
        "tuned_model = hyperparameter_tuning(X_train, y_train, model6, parameters_xgb, 'Halving_Randomized_Search_CV')\n",
        "\n",
        "# Predict on the model\n",
        "tuned_pred = tuned_model.predict(X_test)\n",
        "\n",
        "# Evaluation Metrics after tuning\n",
        "print(f'\\nEvaluation of {type(model6).__name__} after tuning:\\n'+'-----'*10)\n",
        "evaluation_metrics(model6,y_test,tuned_pred)"
      ],
      "metadata": {
        "id": "0RqfXAOCvyWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, tuned_pred)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model1, X_test, y_test, tuned_pred)"
      ],
      "metadata": {
        "id": "ULI5xdR1rLc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "0IiiCUYrvyWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used 'Halving_Randomized_Search_CV' hyperparameter optimization technique because it very less time in comparison to others techniques."
      ],
      "metadata": {
        "id": "vrTtGEhEvyWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "WW_2Owv4vyWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Model i have seen the difference after the tuning and its improvement is \n",
        "\n",
        "accuracy 0.80,\n",
        "\n",
        "precision 0.33, \n",
        "\n",
        "recall 0.58, \n",
        "\n",
        "f1_score 0.42, \n",
        "\n",
        "roc_auc 0.71, \n",
        "\n",
        "log_loss 6.62."
      ],
      "metadata": {
        "id": "ZZxFFRSbvyWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 6 - **Implementing Bagging Classifier**\n",
        "\n"
      ],
      "metadata": {
        "id": "NrM_fW_8wYHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 7 Implementation (Bagging Classifier)\n",
        "model7 = BaggingClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "model7 = model7.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_class_preds = model7.predict(X_train)\n",
        "test_class_preds = model7.predict(X_test)"
      ],
      "metadata": {
        "id": "Jv5lU1wHwYHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "UqjUpcT7wYHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics before tuning\n",
        "print('-----'*23+f'\\nEvaluation of {type(model7).__name__} before tuning:\\n'+'-----'*23)\n",
        "Performance(model7,y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "MiXwgpDTwYHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, test_class_preds)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model7, X_test, y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "-Rk1pWuewYHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "qvoSBGH5wYHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "parameters_bagging = {'n_estimators':[10, 100, 200, 400], \n",
        "                      'random_state':[26]}                    \n",
        "# Fit the Algorithm\n",
        "tuned_model = hyperparameter_tuning(X_train, y_train, model7, parameters_bagging, 'Halving_Randomized_Search_CV')\n",
        "\n",
        "# Predict on the model\n",
        "tuned_pred = tuned_model.predict(X_test)\n",
        "\n",
        "# Evaluation Metrics after tuning\n",
        "print(f'\\nEvaluation of {type(model7).__name__} after tuning:\\n'+'-----'*10)\n",
        "evaluation_metrics(model7,y_test,tuned_pred)"
      ],
      "metadata": {
        "id": "XAJVG2yDwYHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, tuned_pred)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model1, X_test, y_test, tuned_pred)"
      ],
      "metadata": {
        "id": "VbBWcPmVsHy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "s87ADsNJwYHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used 'Halving_Randomized_Search_CV' hyperparameter optimization technique because it very less time in comparison to others techniques."
      ],
      "metadata": {
        "id": "M8Ks5L6IwYHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "HoBpVvqkwYHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this model after the hyperparameter tuning i have slightly difference that is near to negligeble in this model also if we don't do tuning it will work same as before tuning."
      ],
      "metadata": {
        "id": "bFfwFfEzwYHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 7 - **Implementing Lightgbm Classifier**\n",
        "\n"
      ],
      "metadata": {
        "id": "YosHr0qVwaSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 8 Implementation (Lightgbm Classifier)\n",
        "model8 = lgb.LGBMClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "model8 = model8.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_class_preds = model8.predict(X_train)\n",
        "test_class_preds = model8.predict(X_test)"
      ],
      "metadata": {
        "id": "IG3uK27lwaSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "SSWUsRx6waSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics before tuning\n",
        "print('-----'*23+f'\\nEvaluation of {type(model8).__name__} before tuning:\\n'+'-----'*23)\n",
        "Performance(model8,y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "skG4x0NhwaSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, test_class_preds)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model8, X_test, y_test, test_class_preds)"
      ],
      "metadata": {
        "id": "AkgtjMbKwaSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "dLgYx34QwaSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "parameters_lightgbm = {\n",
        "    'max_depths': np.linspace(1, 32, 32, endpoint=True),\n",
        "    'min_data_in_leaf':[100, 200, 250, 300],\n",
        "    'n_estimators':[50,100, 120,150,200],\n",
        "    'learning_rate':[.001,0.01,.1]\n",
        "}                    \n",
        "# Fit the Algorithm\n",
        "tuned_model = hyperparameter_tuning(X_train, y_train, model8, parameters_lightgbm, 'Halving_Randomized_Search_CV')\n",
        "\n",
        "# Predict on the model\n",
        "tuned_pred = tuned_model.predict(X_test)\n",
        "\n",
        "# Evaluation Metrics after tuning\n",
        "print(f'\\nEvaluation of {type(model8).__name__} after tuning:\\n'+'-----'*10)\n",
        "evaluation_metrics(model8,y_test,tuned_pred)"
      ],
      "metadata": {
        "id": "c36oshXKwaSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(X_test, y_test, tuned_pred)\n",
        "# ROC Curve\n",
        "plot_roc_curve(model1, X_test, y_test, tuned_pred)"
      ],
      "metadata": {
        "id": "oHEF90mjtmrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "2el8X8LfwaSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used 'Halving_Randomized_Search_CV' hyperparameter optimization technique because it very less time in comparison to others techniques."
      ],
      "metadata": {
        "id": "JZXey_umwaSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "WNAIhhN7waSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the hyperparameter tuning i have seen the model is under performing, its performing well before tuning . so, no need of tuning in this model."
      ],
      "metadata": {
        "id": "bYPdJFD1waSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evalutaion"
      ],
      "metadata": {
        "id": "wVlygryZRlVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with pd.option_context('display.precision', 10):\n",
        " model_comparison = pd.DataFrame({'Model Name': models,\n",
        "                                                       'Before Tuning Accuracy' : Train_Accuracy,\n",
        "                                                       'After Tuning Accuracy' : Accuracy,\n",
        "                                                       'Before Tuning Precision' : Train_Precision,\n",
        "                                                       'After Tuning Precision' : Precision,\n",
        "                                                       'Before Tuning Recall' : Train_Recall,\n",
        "                                                       'After Tuning Recall' : Recall,\n",
        "                                                       'Before Tuning F1_Score' : Train_F1_Score,\n",
        "                                                       'After Tuning F1_Score' : F1_Score,\n",
        "                                                       'Before Tuning ROC_AUC' : Train_ROC_AUC_Score,\n",
        "                                                       'After Tuning ROC_AUC' : ROC_AUC_Score,\n",
        "                                                       'Before Tuning Log_Loss' : Train_Log_Loss,\n",
        "                                                       'After Tuning Log_Loss' : Log_Loss})\n",
        "model_comparison"
      ],
      "metadata": {
        "id": "CWlIY7MMRwls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the dataframe to a csv to ensure no data loss in working\n",
        "model_comparison.to_csv('/content/drive/MyDrive/Capstone Project - HEALTH INSURANCE CROSS SELL PREDICTION/model.csv')"
      ],
      "metadata": {
        "id": "56DGP7vW8Mik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the dataframe from the csv file again to import the scraped data from the already prepared csv file\n",
        "model_comparison = pd.read_csv('/content/drive/MyDrive/Capstone Project - HEALTH INSURANCE CROSS SELL PREDICTION/model.csv')"
      ],
      "metadata": {
        "id": "05lCA009IM1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_comparison.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
        "model_comparison"
      ],
      "metadata": {
        "id": "NriwZgRlIb2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would like to go with both Recall and Precision and which describes both is F1 Score.\n",
        "\n",
        "To reduce false negative recall is important and to reduce false positives precision is important. Where both are important to be minimized, f1_score is being considered. False Positive is defined as the model predicted that the customer will buy insurance or the customer didn't buy insurance. But according to our model customer will buy insurance so, there would be quite chance of buying insurance not for immediate but after some times. So, for those type of customers we can send them some beneficial modified offers to retain them. Again false negative defines as model will predict that the customer won't buy insurance but the customer really buy. That will be an issue for us. So, for that case we have to minimize the false negative. and false positive we must improve the score of both precision as well as recall which should direclt affect the f1_score positively. So, in our case recall will stand the higher but precision can't be neglected. so, recall should be higher and f1_score should be moderate"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From all the above models that we tried to train and predict the output, we can conclude that **Bagging Classifier** is the best model for our data set. The best parameter of this model is {'n_estimators': 200}. Its Accuracy Score is 0.84, Precision is 0.32, Recall is 0.24, F1_Score is 0.27, ROC_AUC_Score is 0.58 and Log_Loss is 5.28. Its Elapsed time is 4 minute 54 seconds.\n",
        "\n",
        "We can see that we have other models with lower Accuracy Score than Bagging Classifier. as well as Bagging Classifier model, Precision and Recall values are not zero which means True Positives are cannot be zero. That means this model is able to predict correct output if any customer is ready to take vehicle insurance. And as we all know, classification accuracy alone can be misleading if you have an unequal number of observations in each class. This is exactly the case with our data set.\n",
        "\n",
        "Hence, Bagging Classifier* is the best model for our data set.*\n",
        "\n",
        "NOTE: You might get a slight difference in result every time you run because we are using Halving_Randomized_Search_CV to perform hyperparameter tunning which randomly selects the combination of parameters to tune the model."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_plot(importances):\n",
        "    \n",
        "    # Display the five most important features\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    columns = X.columns.values[indices[:5]]\n",
        "    values = importances[indices][:5]\n",
        "\n",
        "    # Creat the plot\n",
        "    fig = plt.figure(figsize = (9,5))\n",
        "    plt.title(\"Normalized Weights for First Five Most Predictive Features\", fontsize = 16)\n",
        "    plt.bar(np.arange(5), values, width = 0.2, align=\"center\", color = '#00A000', \\\n",
        "          label = \"Feature Weight\")\n",
        "    plt.bar(np.arange(5) - 0.2, np.cumsum(values), width = 0.2, align = \"center\", color = '#00A0A0', \\\n",
        "          label = \"Cumulative Feature Weight\")\n",
        "    plt.xticks(np.arange(5), columns)\n",
        "    plt.xlim((-0.5, 4.5))\n",
        "    plt.ylabel(\"Weight\", fontsize = 12)\n",
        "    plt.xlabel(\"Feature\", fontsize = 12)\n",
        "    \n",
        "    plt.legend(loc = 'upper center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2rjvfVp-MH8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_feature_importance():\n",
        "    model = BaggingClassifier(n_estimators=200, random_state=23).fit(X_train,y_train)\n",
        "    \n",
        "    importances = np.mean([\n",
        "        tree.feature_importances_ for tree in model.estimators_\n",
        "        ], axis=0)\n",
        "    feature_plot(importances)"
      ],
      "metadata": {
        "id": "jLCnQecslgdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_feature_importance()"
      ],
      "metadata": {
        "id": "EbEhf87sdGnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "\n",
        "*   Prevously_insured has high impacted and the most in the prediction.\n",
        "*   Vehicle_Age_< 1 year has highest feature weight but in cumulative feature weight.\n",
        "\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting from loading our dataset, we initially checked for null values and duplicates. There were no null values and duplicates so treatment of such was not required. Before data processing, we applied feature scaling techniques to normalize our data to bring all features on the same scale and make it easier to process by ML algorithms.\n",
        "\n",
        "Through Exploratory Data Analysis, \n",
        "\n",
        "Key points:\n",
        "\n",
        "          Male customers are more likely to buy the insurance.\n",
        "\n",
        "          Customers of age between 30 to 60 are more likely to buy insurance.\n",
        "\n",
        "          Customers with Vehicle_Damage are likely to buy insurance.\n",
        "\n",
        "          Customers with Driving License have higher chance of buying Insurance.\n",
        "\n",
        "          Customers of Region Code 0.28 are buying more in comparison to others.\n",
        "\n",
        "          Customers are buying more whose vehicle age in between 1-2 years.\n",
        "\n",
        "          Those customers are more likely to buy who are not buy the insurance  previously.\n",
        "\n",
        "After that we do hypothesis testing and the assumption of our hypothesis statements are right.\n",
        "\n",
        "The variable 'Age', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium', 'Policy_Sales_Channel','Vehicle_Age_1-2 Year','Vehicle_Age_< 1 Year','Vehicle_Age_> 2 Year' are affecting the target variable.\n",
        "For Feature Selection, we used Variance threshold, Extra tree classifier and correlation heatmap . Here we observed that Previously_Insured and vehicle damage is the most important feature and has the highest impact on the dependent feature and there is no correlation between the two numeric features\n",
        "\n",
        "Further, we applied Machine Learning Algorithms to determine whether a customer would be interested in Vehicle Insurance. For the LogisticRegression, GaussianNB, AdaBoostClassifier,XGBClassifier algorithm, we got an accuracy score was obtained around 73%-78%. Similarly, for Decision Tree Classifier, BaggingClassifier, LightGBM accuracy score was obtained around 82%-84%. So, we selected our **BaggingClassifier** as the model with an accuracy score of *84%* considering precision and recall as we have an unequal number of observations in each class in our dataset, so accuracy alone can be misleading.\n",
        "\n",
        "That’s it! We reached the end."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}